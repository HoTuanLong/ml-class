{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.n_in, self.n_out = layers[0].n_in, layers[-1].n_out\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, delta):\n",
    "        for layer in self.layers[::-1]:\n",
    "            delta = layer.backward(delta)\n",
    "        return delta\n",
    "    \n",
    "    def gradient_descent(self, alpha, lr):\n",
    "        for layer in self.layers:\n",
    "            layer.gradient_descent(alpha, lr)\n",
    "            \n",
    "    def reset_velocity(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_velocity()\n",
    "    \n",
    "class MeanSquareLoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, ypred, ytrue):\n",
    "        d = ypred-ytrue\n",
    "        n = len(d)\n",
    "        self.diff_cache = d\n",
    "        return 0.5 * np.sum(d*d) / n\n",
    "    \n",
    "    def backward(self):\n",
    "        '''delta: (B, N_neuron)'''\n",
    "        return self.diff_cache / n\n",
    "\n",
    "class CrossEntropyLossFromLogits:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, ypred, ytrue):\n",
    "        n = ypred.shape[0]\n",
    "        ex = np.exp(ypred-np.max(ypred))\n",
    "        sum_ex = np.sum(ex, axis=1).reshape((-1,1))\n",
    "        act = ex / sum_ex\n",
    "        \n",
    "        log_act = -np.log(act[range(n), ytrue])\n",
    "        \n",
    "        self.cache = (act.copy(), ytrue)\n",
    "        return np.sum(log_act) / n\n",
    "    \n",
    "    def backward(self):\n",
    "        '''delta: (B, N_neuron)'''\n",
    "        act, ytrue = self.cache\n",
    "        n = act.shape[0]\n",
    "        act[range(n), ytrue] -= 1\n",
    "        return act / n\n",
    "\n",
    "    \n",
    "class LinearLayer:\n",
    "    def __init__(self, n_in, n_out, activation_func=\"identity\"):\n",
    "        self.n_in, self.n_out = n_in, n_out\n",
    "        self.activation_func = activation_func\n",
    "        self.W = np.random.randn(n_out, n_in) / n_in\n",
    "        self.b = np.random.randn(n_out, 1) * 0.0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        net = np.matmul( x, self.W.T ) + self.b.T\n",
    "        if self.activation_func == \"sigmoid\":\n",
    "            act = 1. / (1. + np.exp(-net))\n",
    "        elif self.activation_func == \"relu\":\n",
    "            act = np.maximum(net, 0)\n",
    "        else: # identity\n",
    "            act = net\n",
    "        self.cache = (net, act, x)\n",
    "#         print(\"forward\", net.shape, act.shape, x.shape)\n",
    "        return act\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        net, act, x = self.cache\n",
    "        if self.activation_func == \"sigmoid\":\n",
    "            delta = act * (1-act) * delta\n",
    "        elif self.activation_func == \"relu\":\n",
    "            delta = (net>0) * delta\n",
    "        else: # identity or softmax (cross_entropy loss)\n",
    "            delta = delta\n",
    "        # compute derivative\n",
    "        self.dW = np.matmul(delta.T, x)\n",
    "        self.db = np.sum(delta, axis=0).reshape((-1,1))\n",
    "#         print(\"dshape\", self.dW.shape, self.db.shape, delta.shape)\n",
    "        \n",
    "        delta = np.dot(delta, self.W)\n",
    "        return delta\n",
    "    \n",
    "    def gradient_descent(self, alpha, lr):\n",
    "        self.velW = alpha*self.velW - lr*self.dW # use momentum\n",
    "        self.velb = alpha*self.velb - lr*self.db\n",
    "        self.W += self.velW\n",
    "        self.b += self.velb\n",
    "        \n",
    "    def reset_velocity(self):\n",
    "        self.velW = np.zeros_like(self.W)\n",
    "        self.velb = np.zeros_like(self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_net():\n",
    "#     def build_net():\n",
    "#         n1, n2 = 10, 1\n",
    "#         li1 = LinearLayer(n1, n2)\n",
    "#         net = NeuralNet([li1])\n",
    "#         return net\n",
    "\n",
    "#     net = build_net()\n",
    "#     loss_func = MeanSquareLoss()\n",
    "#     batch_size = 100\n",
    "#     alpha, lr = 0.7, 1e-3\n",
    "#     xtr = np.random.randn(batch_size, net.n_in)\n",
    "#     W   = np.random.randn(net.n_out, net.n_in)\n",
    "#     ytr = np.matmul(xtr, W.T)\n",
    "    \n",
    "#     net.reset_velocity()\n",
    "#     history = []\n",
    "#     for epoch in range(1, 1001):\n",
    "#         out  = net.forward(xtr)\n",
    "#         loss = loss_func.forward(out, ytr)\n",
    "        \n",
    "#         delta = loss_func.backward()\n",
    "# #         print(out.shape, delta.shape)\n",
    "#         net.backward(delta)\n",
    "#         net.gradient_descent(alpha, lr)\n",
    "        \n",
    "#         history.append(loss)\n",
    "#         if epoch%100 == 0:\n",
    "#             print(epoch, loss)\n",
    "#     return net, history\n",
    "# net, history = train_net()\n",
    "# plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.28674365799326834 0.925\n",
      "1000 0.15547996266943773 0.9416666666666667\n",
      "1500 0.11337671106978572 0.9472222222222222\n",
      "2000 0.0912502847158644 0.9555555555555556\n",
      "2500 0.07697159643189451 0.9555555555555556\n",
      "3000 0.0666506176317815 0.9555555555555556\n",
      "3500 0.05866887473308407 0.9583333333333334\n",
      "4000 0.05224250387494094 0.9583333333333334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1d804a44eb95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-1d804a44eb95>\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;31m#         print(out.shape, delta.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-79e596f69f7d>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mact\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mact\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_net():\n",
    "    def build_net():\n",
    "        n1, n2, n3 = 64, 32, 10\n",
    "        li1 = LinearLayer(n1, n2, activation_func=\"relu\")\n",
    "        li2 = LinearLayer(n2, n3, activation_func=\"identity\")\n",
    "        net = NeuralNet([li1, li2])\n",
    "        return net\n",
    "\n",
    "    net = build_net()\n",
    "    loss_func = CrossEntropyLossFromLogits()\n",
    "    batch_size = 100\n",
    "    alpha, lr = 0.7, 1e-3\n",
    "    X, y = load_digits(return_X_y=True)\n",
    "    xtr, xte, ytr, yte = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    net.reset_velocity()\n",
    "    history = []\n",
    "    for epoch in range(1, 10001):\n",
    "        out  = net.forward(xtr)\n",
    "        loss = loss_func.forward(out, ytr)\n",
    "        \n",
    "        delta = loss_func.backward()\n",
    "#         print(out.shape, delta.shape)\n",
    "        net.backward(delta)\n",
    "        net.gradient_descent(alpha, lr)\n",
    "        \n",
    "        out = net.forward(xte)\n",
    "        ypred = np.argmax(out, axis=1)\n",
    "        acc = (ypred==yte).sum() / len(yte)\n",
    "        \n",
    "        history.append(loss)\n",
    "        if epoch%500 == 0:\n",
    "            print(epoch, loss, acc)\n",
    "    return net, history\n",
    "net, history = train_net()\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 act=relu\n",
      "layer 1 act=relu\n",
      "layer 2 act=relu\n",
      "layer 3 act=identity\n",
      "epoch 500 , loss 2.270, 45.00%\n",
      "epoch 1000 , loss 1.880, 31.11%\n",
      "epoch 1500 , loss 1.188, 61.67%\n",
      "epoch 2000 , loss 0.749, 76.39%\n",
      "epoch 2500 , loss 0.504, 84.44%\n",
      "epoch 3000 , loss 0.386, 88.33%\n",
      "epoch 3500 , loss 0.306, 91.11%\n",
      "epoch 4000 , loss 0.241, 91.94%\n",
      "epoch 4500 , loss 0.193, 93.33%\n",
      "epoch 5000 , loss 0.161, 94.17%\n",
      "epoch 5500 , loss 0.138, 94.44%\n",
      "epoch 6000 , loss 0.120, 95.83%\n",
      "epoch 6500 , loss 0.104, 95.83%\n",
      "epoch 7000 , loss 0.092, 95.83%\n",
      "epoch 7500 , loss 0.082, 96.11%\n",
      "epoch 8000 , loss 0.073, 96.67%\n",
      "epoch 8500 , loss 0.066, 96.67%\n",
      "epoch 9000 , loss 0.060, 96.39%\n",
      "epoch 9500 , loss 0.054, 96.11%\n",
      "epoch 10000 , loss 0.049, 95.83%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19c33759bc8>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_net():\n",
    "    def build_net():\n",
    "        c = [64, 16, 16, 16, 10]\n",
    "        layers = []\n",
    "        for i in range(len(c)-1):\n",
    "            act = \"identity\" if i >= len(c)-2 else \"relu\"\n",
    "            print(f\"layer {i} act={act}\")\n",
    "            layer = LinearLayer(c[i], c[i+1], activation_func=act)\n",
    "            layers.append(layer)\n",
    "        net = NeuralNet(layers)\n",
    "        return net\n",
    "\n",
    "    net = build_net()\n",
    "    loss_func = CrossEntropyLossFromLogits()\n",
    "    batch_size = 100\n",
    "    alpha, lr = 0.7, 1e-3\n",
    "    X, y = load_digits(return_X_y=True)\n",
    "    xtr, xte, ytr, yte = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    net.reset_velocity()\n",
    "    history = []\n",
    "    for epoch in range(1, 10001):\n",
    "        out  = net.forward(xtr)\n",
    "        loss = loss_func.forward(out, ytr)\n",
    "        \n",
    "        delta = loss_func.backward()\n",
    "#         print(out.shape, delta.shape)\n",
    "        net.backward(delta)\n",
    "        net.gradient_descent(alpha, lr)\n",
    "        \n",
    "        out = net.forward(xte)\n",
    "        ypred = np.argmax(out, axis=1)\n",
    "        acc = (ypred==yte).sum() / len(yte)\n",
    "        \n",
    "        history.append(loss)\n",
    "        if epoch%500 == 0:\n",
    "            print(f\"epoch {epoch} , loss {loss:.3f}, {acc*100:.2f}%\")\n",
    "    return net, history\n",
    "net, history = train_net()\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0cb5df1767fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2666\u001b[0m     \"\"\"\n\u001b[0;32m   2667\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[1;32m-> 2668\u001b[1;33m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "np.max(0, np.array([1,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
