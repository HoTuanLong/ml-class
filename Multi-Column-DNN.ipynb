{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create function to generate dataset at specific size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(target_size):\n",
    "    dataset_transforms = transforms.Compose([\n",
    "        transforms.Resize((target_size, target_size)),\n",
    "        transforms.Pad( (29-target_size) // 2 ),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    return dataset_transforms\n",
    "\n",
    "def get_datasets(target_size=29):\n",
    "    assert target_size <= 29 and target_size % 2 == 1\n",
    "    dataset_transforms = get_transforms(target_size)\n",
    "    train_dataset = datasets.MNIST('./data', train=True, transform=dataset_transforms, target_transform=None, download=True)\n",
    "    \n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    valid_size = len(train_dataset) - train_size\n",
    "    \n",
    "    torch.manual_seed(42) # keep same seed everytime\n",
    "    train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=dataset_transforms, target_transform=None, download=True)\n",
    "    return dict(train=train_dataset, valid=valid_dataset, test=test_dataset)\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    plt.imshow(transforms.ToPILImage()(image), cmap='gray');\n",
    "    \n",
    "def get_dataloaders(data):\n",
    "    dataloaders = dict(\n",
    "        train=DataLoader(data['train'], batch_size=128, shuffle=True, num_workers=6),\n",
    "        valid=DataLoader(data['valid'], batch_size=128, shuffle=False, num_workers=6),\n",
    "        test=DataLoader(data['test'], batch_size=32, shuffle=False, num_workers=6),\n",
    "    )\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_datasets(target_size=21)\n",
    "len(data['train']), len(data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, int, torch.Size([1, 29, 29]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = data['train'][0]\n",
    "type(x), type(y), x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOZ0lEQVR4nO3dX4xUZZrH8d8D9kQbUdpFsAUio8FxNyYLpNGNThQvnCiZBIxZxMQNGmLPxZiMycYscS+WeEU24GauTBgl06OzDmuYiYRsdgbJRIGsBjQsf8fBxXZp7NBoG9s2UUCevejDbg/W+1ZP/TvV/Xw/SYeq89Rb9eTQvzqn+q1zjrm7AEx908puAEBrEHYgCMIOBEHYgSAIOxAEYQeCuKKewWb2gKSfSpou6UV331jl8czzAU3m7lZpudU6z25m0yX9UdL9kgYk7Zf0qLsfy4wh7ECTpcJez278HZI+cPeT7n5O0q8krazj+QA0UT1hnyfp1Lj7A8UyAG2ons/slXYVvrWbbma9knrreB0ADVBP2AckLRh3f76kjy9/kLtvkbRF4jM7UKZ6duP3S1pkZt81s+9IWiNpR2PaAtBoNW/Z3f2CmT0l6bcam3rb6u5HG9YZgIaqeeqtphdjNx5oumZMvQGYRAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IouZLNkuSmfVL+kLSN5IuuHtPI5oC0Hh1hb1wn7t/0oDnAdBE7MYDQdQbdpf0OzN718x6G9EQgOaodzf+bnf/2MzmSNplZn9w97fGP6B4E+CNACiZuXtjnshsg6RRd9+UeUxjXgxAkrtbpeU178ab2Qwzm3nptqQfSDpS6/MBaK56duPnSvqNmV16nn919/9oSFdoW8X/d0XXXnttduzNN9+crD3xxBPZsbfeemuy9skn+cmg5557Lll7//33s2OnkprD7u4nJf11A3sB0ERMvQFBEHYgCMIOBEHYgSAIOxAEYQeCaMRRb5hkpk1Lv8d3dnZmx952223J2tq1a7Njly9fnqzl5tElqaOjI1kbGhrKjn3llVeStRMnTmTHXrx4MVufTNiyA0EQdiAIwg4EQdiBIAg7EARhB4Jg6m0K6urqytZXrVqVrC1btiw79t57703WFi1alB17xRXN+XXLTctJ0qxZs5K16dOnZ8cy9QZg0iHsQBCEHQiCsANBEHYgCMIOBEHYgSCYZ29T1eakly5dmqytXr06O/axxx5L1ubMmZNvrA65C5KMjo5mx547dy5Z27NnT3bsvn37krXz589nx04lbNmBIAg7EARhB4Ig7EAQhB0IgrADQVSdejOzrZJ+KGnI3W8vll0naZukhZL6Ja1298+a1+bUdNVVVyVrDz/8cHbsk08+maz19PRkx1555ZX5xjJyh3wODw9nx545cyZZ27RpU3bs0aNHk7WzZ89mx54+fTpbj2IiW/afS3rgsmXrJe1290WSdhf3AbSxqmF397ckXf6WvVJSX3G7T1L6bAgA2kKtn9nnuvugJBX/Nu9rVwAaoulflzWzXkm9zX4dAHm1btnPmFm3JBX/Jq+/4+5b3L3H3fN/NQLQVLWGfYekSxf2Wivp9ca0A6BZqobdzF6V9J+SvmdmA2a2TtJGSfeb2QlJ9xf3AbQxyx122PAXM2vdi00CCxYsSNb279+fHdusQ1FHRkay9bfffjtZe+2117Jjc3PlBw8ezI796quvsnX8P3e3Ssv5Bh0QBGEHgiDsQBCEHQiCsANBEHYgCM4uW6fcVUDvvPPO7Nh169Yla9dcc03NPVU7Y2rukM/t27dnx/b19SVrH330UXbshQsXkrVqU4lff/11svb5559nx+bOTDuVrtJaDVt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCQ1zr1NXVlaxt3Jg/zP/xxx9P1jo6OrJjc/PDe/fuzY598cUXk7XOzs7s2NxhufX8Lt13333Z+qeffpqs5Q67laSdO3cma8eOHcuOnYzz8BziCgRH2IEgCDsQBGEHgiDsQBCEHQiCQ1zrNHfu3GRt6dKl2bHVptdyctNcn32Wv6DuI488kqxVOyx39uzZ+cZK8OCDD2bry5YtS9aeeeaZ7NiTJ0/W1FM7YssOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0FUnWc3s62SfihpyN1vL5ZtkPSkpLPFw551939vVpPtbP78+cnakiVLmva6uVNYr1ixouax06ZNvvf/at9XuOeee5K13CG7Urx59p9LeqDC8n9x98XFT8igA5NJ1bC7+1uShlvQC4Amqmef7SkzO2RmW80sfboWAG2h1rC/IOkWSYslDUranHqgmfWa2QEzO1DjawFogJrC7u5n3P0bd78o6WeS7sg8dou797h7T61NAqhfTWE3s+5xdx+SdKQx7QBololMvb0qabmk2WY2IOmfJC03s8WSXFK/pB81sce2ZlbxRJ6S8lNcUn1nY82pNhWVe93c1VKl/JVaz549m6xJUnd3d7JWbQqsnsOBc2eIbeXZlctWNezu/miFxS81oRcATTT5vkEBoCaEHQiCsANBEHYgCMIOBEHYgSA4lXSdvvzyy2St2rxzWadlzs07Hz58ODt227ZtyVpuXUjSmjVrkrUbb7wxO7aeefZDhw4lawMDAzU/72TDlh0IgrADQRB2IAjCDgRB2IEgCDsQBFNvdfrwww+TtT179mTHrlq1KlnLHTpbr9wZZG+66abs2N7e3mRt5syZ2bE33HBDvrGMc+fOJWunT5/Ojn355ZeTtVOnTtXc02TDlh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCevU4jIyPJ2t69e7Nj77rrrmRtzpw52bH1zMPnxl5//fXZsdXqtao2V75r165kbfPm5AWJJOWvxHr+/Pl8Y1MIW3YgCMIOBEHYgSAIOxAEYQeCIOxAEBO5iusCSb+QdIOki5K2uPtPzew6SdskLdTYlVxXu/tnzWu1PeXOqNrX15cd29nZmaytX78+O3bGjBn5xpokd9XT3BSXJO3bty9Ze+ONN7Jj33zzzWQt0mGq9ZjIlv2CpL9397+U9DeSfmxmfyVpvaTd7r5I0u7iPoA2VTXs7j7o7u8Vt7+QdFzSPEkrJV3adPVJSp+JAUDp/qzP7Ga2UNISSe9Imuvug9LYG4Kk/Fe+AJRqwl+XNbOrJW2X9LS7j0z065pm1ispfS4jAC0xoS27mXVoLOi/dPdfF4vPmFl3Ue+WNFRprLtvcfced+9pRMMAalM17Da2CX9J0nF3f35caYektcXttZJeb3x7ABplIrvxd0v6O0mHzexgsexZSRsl/ZuZrZP0P5L+tjktAmgEy82bNvzFzFr3YpPArFmzkrXFixdnx9ZzVdN65H5fhoeHs2P7+/uTtdHR0ezY3Kmk8afcveIf1PgGHRAEYQeCIOxAEIQdCIKwA0EQdiAIpt6AKYapNyA4wg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEBO5PvsCM/u9mR03s6Nm9pNi+QYzO21mB4ufFc1vF0Ctqp433sy6JXW7+3tmNlPSu5JWSVotadTdN034xThvPNB0qfPGXzGBgYOSBovbX5jZcUnzGtsegGb7sz6zm9lCSUskvVMsesrMDpnZVjPranBvABpowmE3s6slbZf0tLuPSHpB0i2SFmtsy785Ma7XzA6Y2YEG9AugRhO61puZdUjaKem37v58hfpCSTvd/fYqz8NndqDJar7Wm5mZpJckHR8f9OIPd5c8JOlIvU0CaJ6J/DX++5L2SDos6WKx+FlJj2psF94l9Uv6UfHHvNxzsWUHmiy1ZeeSzcAUwyWbgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQii6rXeGuwTSR+Nuz+7WNZO2rEnqT37aseepPbsq1U93ZQqtPRU0t96cbMD7t5TWgMVtGNPUnv21Y49Se3ZVzv0xG48EARhB4IoO+xbSn79StqxJ6k9+2rHnqT27Kv0nkr9zA6gdcresgNokVLCbmYPmNn7ZvaBma0vo4dKzKzfzA6b2UEzO1BSD1vNbMjMjoxbdp2Z7TKzE8W/XW3S1wYzO12sr4NmtqLFPS0ws9+b2XEzO2pmPymWl7q+Mn2Vu75avRtvZtMl/VHS/ZIGJO2X9Ki7H2tpIxWYWb+kHncvbY7WzO6RNCrpF+5+e7HsnyUNu/vG4s2xy93/oQ362iBp1N03tbKXcT11S+p29/fMbKakdyWtkvS4Slxfmb5Wq8T1VcaW/Q5JH7j7SXc/J+lXklaW0Edbcve3JA1ftnilpL7idp/GfnFaKtFXqdx90N3fK25/Iem4pHkqeX1l+ipVGWGfJ+nUuPsDaoMVUXBJvzOzd82st+xmxpnr7oPS2C+SpDkl9zPeU2Z2qNjNb/nHi0vMbKGkJZLeURutr8v6kkpcX2WEvdKF4ttlSuBud18q6UFJPy52XZH2gqRbJC2WNChpcxlNmNnVkrZLetrdR8rooZIKfZW6vsoI+4CkBePuz5f0cQl9fIu7f1z8OyTpNxr7yNEOzhSfAy99HhwquR9Jkrufcfdv3P2ipJ+phPVlZh0aC9Qv3f3XxeLS11elvspeX2WEfb+kRWb2XTP7jqQ1knaU0MefMLMZxR9TZGYzJP1A0pH8qJbZIWltcXutpNdL7OX/XApU4SG1eH2ZmUl6SdJxd39+XKnU9ZXqq+z1JXdv+Y+kFRr7i/x/S/rHMnqo0NPNkv6r+DlaVl+SXtXYLt55je0FrZP0F5J2SzpR/Htdm/T1sqTDkg5pLGDdLe7p+xr7CHhI0sHiZ0XZ6yvTV6nri2/QAUHwDTogCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0H8Ly0dboSvLJuEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_tensor_image(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create model class and get_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer torch.Size([128, 20, 26, 26])\n",
      "second layer torch.Size([128, 20, 13, 13])\n",
      "third layer torch.Size([128, 40, 9, 9])\n",
      "fourth layer torch.Size([128, 40, 3, 3])\n",
      "final layer torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "class MCDNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCDNN1, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "class MCDNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCDNN2, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=2)),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "class MCDNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCDNN3, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=2)),\n",
    "            ('conv2', nn.Conv2d(in_channels=20, out_channels=40, kernel_size=5)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "class MCDNN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCDNN4, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=2)),\n",
    "            ('conv2', nn.Conv2d(in_channels=20, out_channels=40, kernel_size=5)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=3)),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, start_dim=1)\n",
    "    \n",
    "class MCDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCDNN, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=2)),\n",
    "            ('conv2', nn.Conv2d(in_channels=20, out_channels=40, kernel_size=5)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=3)),\n",
    "            ('flatten', Flatten()),\n",
    "            ('fc1', nn.Linear(in_features=40*3*3, out_features=150)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(in_features=150, out_features=10)),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "data = get_datasets(target_size=21)\n",
    "dataloaders = get_dataloaders(data)\n",
    "batch_x, batch_y = next(iter(dataloaders['train']))\n",
    "print('first layer', MCDNN1()(batch_x).shape)\n",
    "print('second layer', MCDNN2()(batch_x).shape)\n",
    "print('third layer', MCDNN3()(batch_x).shape)\n",
    "print('fourth layer', MCDNN4()(batch_x).shape)\n",
    "print('final layer', MCDNN()(batch_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create training function to train a model given ```dataloaders['train']``` and ```dataloaders['valid']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, model, loss_func, optimizer, scheduler=None, device=\"cuda:0\", epochs=1, save_func=None):\n",
    "    \n",
    "    def train_epoch():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in dataloaders['train']:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            pred = model(batch_x)\n",
    "            loss = loss_func(pred, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None: scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(dataloaders['train'])\n",
    "            \n",
    "    def valid_epoch():\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in dataloaders['valid']:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                pred = model(batch_x)\n",
    "                loss = loss_func(pred, batch_y)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(dataloaders['valid'])\n",
    "    \n",
    "    train_loss, valid_loss = [], []\n",
    "    best_valid_loss = float('inf')\n",
    "    pbar = tqdm(range(epochs), total=epochs)\n",
    "    save_message = ''\n",
    "    for epoch in pbar:\n",
    "        train_loss.append(train_epoch())\n",
    "        valid_loss.append(valid_epoch())\n",
    "        pbar.set_description(f'epoch {epoch} current best {best_valid_loss:.3f} {save_message}')\n",
    "        if valid_loss[-1] < best_valid_loss:\n",
    "            best_valid_loss = valid_loss[-1]\n",
    "            if save_func is not None:\n",
    "                save_message = save_func(model)\n",
    "                pbar.set_description(f'epoch {epoch} new best {valid_loss[-1]:.3f} {save_message}')\n",
    "    return train_loss, valid_loss\n",
    "                \n",
    "def save_func(target_size, column):\n",
    "                \n",
    "    def __save_func(model):\n",
    "        model_path = f'./models/model_{target_size}_{column}.pth'\n",
    "        torch.save(dict(state_dict=model.state_dict()), model_path)\n",
    "        return f'{model_path}'\n",
    "    \n",
    "    return __save_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 new best 0.073 ./models/model_29_10.pth: 100%|██████████| 2/2 [00:08<00:00,  4.41s/it]    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.33361871617039046, 0.08121874349812666],\n",
       " [0.10787677602406512, 0.07261109811828491])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_datasets(target_size=29)\n",
    "dataloaders = get_dataloaders(data)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "epochs = 2\n",
    "model = MCDNN().to(device)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "train_model(dataloaders, model, loss_func, optimizer, epochs=epochs, device=device, save_func=save_func(29, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train all models and save best models to ```./models``` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZES = [29, 27, 25, 21, 19, 17, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 99 current best 0.031 ./models/model_29_0.pth: 100%|██████████| 100/100 [06:55<00:00,  4.15s/it]\n",
      "epoch 99 current best 0.032 ./models/model_29_1.pth: 100%|██████████| 100/100 [06:58<00:00,  4.19s/it]\n",
      "epoch 99 current best 0.031 ./models/model_29_2.pth: 100%|██████████| 100/100 [07:05<00:00,  4.25s/it]\n",
      "epoch 99 current best 0.030 ./models/model_29_3.pth: 100%|██████████| 100/100 [07:03<00:00,  4.23s/it]\n",
      "epoch 99 current best 0.031 ./models/model_29_4.pth: 100%|██████████| 100/100 [06:59<00:00,  4.19s/it]\n",
      "epoch 99 current best 0.027 ./models/model_27_0.pth: 100%|██████████| 100/100 [07:00<00:00,  4.21s/it]\n",
      "epoch 99 current best 0.031 ./models/model_27_1.pth: 100%|██████████| 100/100 [06:58<00:00,  4.18s/it]\n",
      "epoch 99 current best 0.029 ./models/model_27_2.pth: 100%|██████████| 100/100 [07:07<00:00,  4.27s/it]\n",
      "epoch 99 current best 0.029 ./models/model_27_3.pth: 100%|██████████| 100/100 [07:03<00:00,  4.24s/it]\n",
      "epoch 99 new best 0.029 ./models/model_27_4.pth: 100%|██████████| 100/100 [06:57<00:00,  4.17s/it]   \n",
      "epoch 99 current best 0.029 ./models/model_25_0.pth: 100%|██████████| 100/100 [06:56<00:00,  4.17s/it]\n",
      "epoch 99 current best 0.030 ./models/model_25_1.pth: 100%|██████████| 100/100 [07:00<00:00,  4.20s/it]\n",
      "epoch 99 current best 0.030 ./models/model_25_2.pth: 100%|██████████| 100/100 [06:58<00:00,  4.18s/it]\n",
      "epoch 99 current best 0.029 ./models/model_25_3.pth: 100%|██████████| 100/100 [06:54<00:00,  4.14s/it]\n",
      "epoch 99 current best 0.030 ./models/model_25_4.pth: 100%|██████████| 100/100 [06:59<00:00,  4.19s/it]\n",
      "epoch 99 current best 0.028 ./models/model_21_0.pth: 100%|██████████| 100/100 [06:57<00:00,  4.18s/it]\n",
      "epoch 99 current best 0.031 ./models/model_21_1.pth: 100%|██████████| 100/100 [06:57<00:00,  4.17s/it]\n",
      "epoch 99 current best 0.030 ./models/model_21_2.pth: 100%|██████████| 100/100 [07:01<00:00,  4.22s/it]\n",
      "epoch 99 current best 0.029 ./models/model_21_3.pth: 100%|██████████| 100/100 [07:02<00:00,  4.23s/it]\n",
      "epoch 99 current best 0.028 ./models/model_21_4.pth: 100%|██████████| 100/100 [06:58<00:00,  4.18s/it]\n",
      "epoch 99 current best 0.034 ./models/model_19_0.pth: 100%|██████████| 100/100 [07:00<00:00,  4.20s/it]\n",
      "epoch 99 current best 0.032 ./models/model_19_1.pth: 100%|██████████| 100/100 [07:03<00:00,  4.24s/it]\n",
      "epoch 99 current best 0.033 ./models/model_19_2.pth: 100%|██████████| 100/100 [06:59<00:00,  4.19s/it]\n",
      "epoch 99 current best 0.031 ./models/model_19_3.pth: 100%|██████████| 100/100 [07:04<00:00,  4.25s/it]\n",
      "epoch 99 current best 0.033 ./models/model_19_4.pth: 100%|██████████| 100/100 [06:57<00:00,  4.18s/it]\n",
      "epoch 99 current best 0.035 ./models/model_17_0.pth: 100%|██████████| 100/100 [06:58<00:00,  4.19s/it]\n",
      "epoch 99 current best 0.036 ./models/model_17_1.pth: 100%|██████████| 100/100 [06:59<00:00,  4.20s/it]\n",
      "epoch 99 current best 0.038 ./models/model_17_2.pth: 100%|██████████| 100/100 [06:59<00:00,  4.20s/it]\n",
      "epoch 99 current best 0.037 ./models/model_17_3.pth: 100%|██████████| 100/100 [06:57<00:00,  4.17s/it]\n",
      "epoch 99 current best 0.035 ./models/model_17_4.pth: 100%|██████████| 100/100 [06:55<00:00,  4.15s/it]\n",
      "epoch 99 current best 0.039 ./models/model_15_0.pth: 100%|██████████| 100/100 [07:01<00:00,  4.21s/it]\n",
      "epoch 99 current best 0.039 ./models/model_15_1.pth: 100%|██████████| 100/100 [07:00<00:00,  4.20s/it]\n",
      "epoch 99 current best 0.039 ./models/model_15_2.pth: 100%|██████████| 100/100 [07:04<00:00,  4.25s/it]\n",
      "epoch 99 current best 0.036 ./models/model_15_3.pth: 100%|██████████| 100/100 [07:01<00:00,  4.22s/it]\n",
      "epoch 99 current best 0.038 ./models/model_15_4.pth: 100%|██████████| 100/100 [06:59<00:00,  4.19s/it]\n"
     ]
    }
   ],
   "source": [
    "### comment/uncomment the following code for training all models\n",
    "\n",
    "for target_size in TARGET_SIZES:\n",
    "    for column in range(5):\n",
    "        data = get_datasets(target_size=target_size)\n",
    "        dataloaders = get_dataloaders(data)\n",
    "\n",
    "        device = \"cuda:0\"\n",
    "        epochs = 100\n",
    "        model = MCDNN().to(device)\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "        train_model(dataloaders, model, loss_func, optimizer, epochs=epochs, device=device, save_func=save_func(target_size, column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate (generate table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate probability column of a model\n",
    "def get_column(input_image, model, target_size, device):\n",
    "    transform = get_transforms(target_size)\n",
    "    input_tensor = transform(input_image)[None,...]\n",
    "    with torch.no_grad():\n",
    "        column = model(input_tensor.to(device)).cpu()\n",
    "    return column\n",
    "\n",
    "def get_error_rate(prediction, ground_truth):\n",
    "    assert len(prediction) == len(ground_truth)\n",
    "    n_error = sum([1 if x != y else 0 for x, y in zip(prediction, ground_truth)])\n",
    "    return n_error / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.041 sec\n",
      "500 19.089 sec\n",
      "1000 38.394 sec\n",
      "1500 57.718 sec\n",
      "2000 77.112 sec\n",
      "2500 96.160 sec\n",
      "3000 115.590 sec\n",
      "3500 134.769 sec\n",
      "4000 154.533 sec\n",
      "4500 174.191 sec\n",
      "5000 193.248 sec\n",
      "5500 212.357 sec\n",
      "6000 231.860 sec\n",
      "6500 250.940 sec\n",
      "7000 269.997 sec\n",
      "7500 289.543 sec\n",
      "8000 309.365 sec\n",
      "8500 328.832 sec\n",
      "9000 347.954 sec\n",
      "9500 367.071 sec\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset without any transform, we will transform using above helper function\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=None, target_transform=None, download=True)\n",
    "\n",
    "# Load all 35 saved models\n",
    "models = {}\n",
    "device = \"cuda:0\"\n",
    "for target_size in TARGET_SIZES:\n",
    "    for column in range(5):\n",
    "        model_id = f'{target_size}_{column}'\n",
    "        model_path = f'./models/model_{model_id}.pth'\n",
    "        model = MCDNN()\n",
    "        model.load_state_dict(torch.load(model_path)['state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        models[model_id] = model\n",
    "\n",
    "# generate all predictions on test dataset\n",
    "ground_truth = [y for x, y in test_dataset]\n",
    "predictions = {k: [] for k, v in models.items()}\n",
    "predictions.update({f'{target_size}': [] for target_size in TARGET_SIZES})\n",
    "predictions['all'] = []\n",
    "\n",
    "start_time = time.time()\n",
    "for idx, (x, y) in enumerate(test_dataset):\n",
    "    columns = {k: None for k, v in models.items()}\n",
    "    all_net_avg = 0\n",
    "    ### switch between the following two lines to test and to run all models\n",
    "    # for target_size in [29]: #, 27, 25, 21, 19, 17, 15]: \n",
    "    for target_size in TARGET_SIZES:\n",
    "        five_net_avg = 0\n",
    "        for column in range(5):\n",
    "            model_id = f'{target_size}_{column}'\n",
    "            column = get_column(x, models[model_id], target_size, device)\n",
    "            columns[model_id] = column\n",
    "            predictions[model_id].append( torch.argmax(column).item() ) # argmax works here because there is only ONE sample\n",
    "            \n",
    "            five_net_avg += column\n",
    "            all_net_avg += column\n",
    "\n",
    "        five_net_avg /= 5 # take 5-column average\n",
    "        predictions[f'{target_size}'].append( torch.argmax(five_net_avg).item() )\n",
    "\n",
    "    all_net_avg /= 35 # take 35-column average\n",
    "    predictions['all'].append( torch.argmax(all_net_avg).item() )\n",
    "    \n",
    "    if idx % 500 == 0:\n",
    "        print(idx, f'{time.time()-start_time:.3f} sec')\n",
    "\n",
    "## save results after a long experiment so that we could run analysis later\n",
    "## always save raw results so that we could run different analysis later\n",
    "with open('./results/predictions.pkl', 'wb') as f:\n",
    "    pickle.dump((predictions, ground_truth), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29_0 0.72%\n",
      "29_1 0.78%\n",
      "29_2 0.77%\n",
      "29_3 0.87%\n",
      "29_4 0.87%\n",
      "27_0 0.71%\n",
      "27_1 0.66%\n",
      "27_2 0.76%\n",
      "27_3 0.70%\n",
      "27_4 0.63%\n",
      "25_0 0.90%\n",
      "25_1 0.77%\n",
      "25_2 0.81%\n",
      "25_3 0.68%\n",
      "25_4 0.76%\n",
      "21_0 0.79%\n",
      "21_1 0.77%\n",
      "21_2 0.85%\n",
      "21_3 0.79%\n",
      "21_4 0.78%\n",
      "19_0 0.94%\n",
      "19_1 0.83%\n",
      "19_2 0.85%\n",
      "19_3 0.75%\n",
      "19_4 0.85%\n",
      "17_0 0.86%\n",
      "17_1 0.91%\n",
      "17_2 0.83%\n",
      "17_3 0.95%\n",
      "17_4 0.88%\n",
      "15_0 0.81%\n",
      "15_1 0.91%\n",
      "15_2 0.92%\n",
      "15_3 0.81%\n",
      "15_4 0.78%\n",
      "29 0.69%\n",
      "27 0.58%\n",
      "25 0.74%\n",
      "21 0.66%\n",
      "19 0.72%\n",
      "17 0.77%\n",
      "15 0.66%\n",
      "all 0.58%\n"
     ]
    }
   ],
   "source": [
    "# load experiment results to run analysis, independent from above code \n",
    "with open('./results/predictions.pkl', 'rb') as f:\n",
    "    predictions, ground_truth = pickle.load(f)\n",
    "\n",
    "def print_error_rate(model_id):\n",
    "    error_rate = get_error_rate(predictions[model_id], ground_truth)\n",
    "    print(f'{model_id} {error_rate*100:.2f}%')\n",
    "\n",
    "for target_size in TARGET_SIZES:\n",
    "    for column in range(5):\n",
    "        print_error_rate( f'{target_size}_{column}' )\n",
    "        \n",
    "for target_size in TARGET_SIZES:\n",
    "    print_error_rate( f'{target_size}' )\n",
    "\n",
    "print_error_rate( 'all' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
