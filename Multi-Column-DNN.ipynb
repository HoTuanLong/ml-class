{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create function to generate dataset at specific size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(target_size, is_train=True):\n",
    "    trans = [\n",
    "        transforms.Resize((target_size, target_size)),\n",
    "        transforms.Pad( (29-target_size) // 2 ),\n",
    "    ]\n",
    "    if is_train:\n",
    "        trans += [\n",
    "            transforms.RandomApply([transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)], p=0.6),\n",
    "            transforms.RandomApply([transforms.RandomAffine(10, translate=(0.1,0.1), scale=(0.9,1.1), shear=3)], p=0.6),\n",
    "        ]\n",
    "    trans.append(transforms.ToTensor())\n",
    "    dataset_transforms = transforms.Compose(trans)\n",
    "    return dataset_transforms\n",
    "\n",
    "def get_datasets(target_size=29):\n",
    "    assert target_size <= 29 and target_size % 2 == 1\n",
    "\n",
    "    train_transforms = get_transforms(target_size)\n",
    "    train_dataset = datasets.MNIST('./data', train=True, transform=train_transforms, target_transform=None, download=True)\n",
    "    \n",
    "    torch.manual_seed(42) # keep same seed everytime\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    valid_size = len(train_dataset) - train_size    \n",
    "    train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n",
    "\n",
    "    test_transforms = get_transforms(target_size, is_train=False)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=test_transforms, target_transform=None, download=True)\n",
    "\n",
    "    return dict(train=train_dataset, valid=valid_dataset, test=test_dataset)\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    plt.imshow(transforms.ToPILImage()(image), cmap='gray');\n",
    "    \n",
    "def get_dataloaders(data):\n",
    "    dataloaders = dict(\n",
    "        train=DataLoader(data['train'], batch_size=256, shuffle=True, num_workers=6),\n",
    "        valid=DataLoader(data['valid'], batch_size=128, shuffle=False, num_workers=6),\n",
    "        test=DataLoader(data['test'], batch_size=32, shuffle=False, num_workers=6),\n",
    "    )\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_datasets(target_size=21)\n",
    "len(data['train']), len(data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, int, torch.Size([1, 29, 29]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = data['train'][0]\n",
    "type(x), type(y), x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_tensor_image(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create model class and get_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer torch.Size([256, 20, 26, 26])\n",
      "second layer torch.Size([256, 20, 13, 13])\n",
      "third layer torch.Size([256, 40, 9, 9])\n",
      "fourth layer torch.Size([256, 40, 3, 3])\n",
      "final layer torch.Size([256, 10])\n"
     ]
    }
   ],
   "source": [
    "class MCDNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCDNN1, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "class MCDNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCDNN2, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=2)),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "class MCDNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCDNN3, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=2)),\n",
    "            ('conv2', nn.Conv2d(in_channels=20, out_channels=40, kernel_size=5)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "class MCDNN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCDNN4, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=2)),\n",
    "            ('conv2', nn.Conv2d(in_channels=20, out_channels=40, kernel_size=5)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=3)),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, start_dim=1)\n",
    "\n",
    "class ScaledTanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledTanh, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x*torch.tanh(x)\n",
    "\n",
    "\n",
    "class MCDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCDNN, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4)),\n",
    "            ('relu1', ScaledTanh()),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=2)),\n",
    "            ('conv2', nn.Conv2d(in_channels=20, out_channels=40, kernel_size=5)),\n",
    "            ('relu2', ScaledTanh()),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=3)),\n",
    "            ('flatten', Flatten()),\n",
    "            ('fc1', nn.Linear(in_features=40*3*3, out_features=150)),\n",
    "            ('relu3', ScaledTanh()),\n",
    "            ('fc2', nn.Linear(in_features=150, out_features=10)),\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "data = get_datasets(target_size=21)\n",
    "dataloaders = get_dataloaders(data)\n",
    "batch_x, batch_y = next(iter(dataloaders['train']))\n",
    "print('first layer', MCDNN1()(batch_x).shape)\n",
    "print('second layer', MCDNN2()(batch_x).shape)\n",
    "print('third layer', MCDNN3()(batch_x).shape)\n",
    "print('fourth layer', MCDNN4()(batch_x).shape)\n",
    "print('final layer', MCDNN()(batch_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create training function to train a model given ```dataloaders['train']``` and ```dataloaders['valid']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, model, loss_func, optimizer, scheduler=None, device=\"cuda:0\", epochs=1, save_func=None):\n",
    "    \n",
    "    def train_epoch():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in dataloaders['train']:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            pred = model(batch_x)\n",
    "            loss = loss_func(pred, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None: scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(dataloaders['train'])\n",
    "            \n",
    "    def valid_epoch():\n",
    "        model.eval()\n",
    "        total_loss, total_error, total_count = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in dataloaders['valid']:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                pred = model(batch_x)\n",
    "                loss = loss_func(pred, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                y_pred = torch.argmax(pred, dim=1)\n",
    "                total_error += torch.sum(y_pred != batch_y).item()\n",
    "                total_count += batch_y.shape[0]\n",
    "\n",
    "        return total_loss / len(dataloaders['valid']), total_error / total_count\n",
    "    \n",
    "    train_loss, valid_loss, valid_error = [], [], []\n",
    "    best_valid_loss = float('inf')\n",
    "    pbar = tqdm(range(epochs), total=epochs)\n",
    "    save_message = ''\n",
    "    for epoch in pbar:\n",
    "        train_loss.append(train_epoch())\n",
    "        vloss, verror = valid_epoch()\n",
    "        valid_loss.append(vloss)\n",
    "        valid_error.append(verror)\n",
    "        pbar.set_description(f'epoch {epoch} current best {best_valid_loss*100:.3f}% error {verror*100:.3f}% {save_message}')\n",
    "        if valid_error[-1] < best_valid_loss:\n",
    "            best_valid_loss = valid_error[-1]\n",
    "            if save_func is not None:\n",
    "                save_message = save_func(model)\n",
    "                pbar.set_description(f'epoch {epoch} new best {best_valid_loss*100:.3f}% error {verror*100:.3f}% {save_message}')\n",
    "    return train_loss, valid_loss, valid_error\n",
    "                \n",
    "def save_func(target_size, column):\n",
    "                \n",
    "    def __save_func(model):\n",
    "        model_path = f'./models/model_{target_size}_{column}.pth'\n",
    "        torch.save(dict(state_dict=model.state_dict()), model_path)\n",
    "        return f'{model_path}'\n",
    "    \n",
    "    return __save_func\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    if type(m) == nn.Conv2D:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 new best 3.725% error 3.725% ./models/model_29_10.pth: 100%|██████████| 2/2 [00:11<00:00,  5.99s/it]    \n"
     ]
    }
   ],
   "source": [
    "data = get_datasets(target_size=29)\n",
    "dataloaders = get_dataloaders(data)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "epochs = 2\n",
    "model = MCDNN().to(device)\n",
    "model.apply(init_weights)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_model(dataloaders, model, loss_func, optimizer, epochs=epochs, device=device, save_func=save_func(29, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train all models and save best models to ```./models``` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZES = [29, 27, 25, 21, 19, 17, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 99 current best 0.758% error 0.858% ./models/model_29_0.pth: 100%|██████████| 100/100 [09:27<00:00,  5.68s/it]\n",
      "epoch 99 current best 0.725% error 0.858% ./models/model_29_1.pth: 100%|██████████| 100/100 [09:34<00:00,  5.75s/it]\n",
      "epoch 99 current best 0.817% error 0.833% ./models/model_29_2.pth: 100%|██████████| 100/100 [09:27<00:00,  5.68s/it]\n",
      "epoch 99 current best 0.783% error 0.833% ./models/model_29_3.pth: 100%|██████████| 100/100 [09:33<00:00,  5.73s/it]\n",
      "epoch 99 current best 0.800% error 0.867% ./models/model_29_4.pth: 100%|██████████| 100/100 [09:33<00:00,  5.74s/it]\n",
      "epoch 99 current best 0.842% error 0.933% ./models/model_27_0.pth: 100%|██████████| 100/100 [09:24<00:00,  5.64s/it]\n",
      "epoch 99 current best 0.742% error 0.933% ./models/model_27_1.pth: 100%|██████████| 100/100 [09:28<00:00,  5.69s/it]\n",
      "epoch 99 current best 0.750% error 0.950% ./models/model_27_2.pth: 100%|██████████| 100/100 [09:27<00:00,  5.67s/it]\n",
      "epoch 99 current best 0.742% error 0.925% ./models/model_27_3.pth: 100%|██████████| 100/100 [09:31<00:00,  5.71s/it]\n",
      "epoch 99 current best 0.708% error 0.958% ./models/model_27_4.pth: 100%|██████████| 100/100 [09:37<00:00,  5.77s/it]\n",
      "epoch 99 current best 0.792% error 1.000% ./models/model_25_0.pth: 100%|██████████| 100/100 [09:35<00:00,  5.75s/it]\n",
      "epoch 99 current best 0.767% error 0.983% ./models/model_25_1.pth: 100%|██████████| 100/100 [09:34<00:00,  5.75s/it]\n",
      "epoch 99 current best 0.775% error 1.033% ./models/model_25_2.pth: 100%|██████████| 100/100 [09:33<00:00,  5.74s/it]\n",
      "epoch 99 current best 0.775% error 0.933% ./models/model_25_3.pth: 100%|██████████| 100/100 [09:37<00:00,  5.77s/it]\n",
      "epoch 99 current best 0.783% error 0.967% ./models/model_25_4.pth: 100%|██████████| 100/100 [09:35<00:00,  5.76s/it]\n",
      "epoch 99 current best 0.817% error 1.017% ./models/model_21_0.pth: 100%|██████████| 100/100 [09:35<00:00,  5.76s/it]\n",
      "epoch 99 current best 0.867% error 1.183% ./models/model_21_1.pth: 100%|██████████| 100/100 [09:35<00:00,  5.75s/it]\n",
      "epoch 99 current best 0.833% error 1.033% ./models/model_21_2.pth: 100%|██████████| 100/100 [09:39<00:00,  5.79s/it]\n",
      "epoch 99 current best 0.792% error 0.975% ./models/model_21_3.pth: 100%|██████████| 100/100 [09:24<00:00,  5.65s/it]\n",
      "epoch 99 current best 0.817% error 1.033% ./models/model_21_4.pth: 100%|██████████| 100/100 [09:26<00:00,  5.67s/it]\n",
      "epoch 99 current best 1.000% error 1.342% ./models/model_19_0.pth: 100%|██████████| 100/100 [09:19<00:00,  5.60s/it]\n",
      "epoch 99 current best 1.017% error 1.258% ./models/model_19_1.pth: 100%|██████████| 100/100 [09:15<00:00,  5.55s/it]\n",
      "epoch 99 current best 0.975% error 1.108% ./models/model_19_2.pth: 100%|██████████| 100/100 [09:14<00:00,  5.54s/it]\n",
      "epoch 99 current best 0.975% error 1.233% ./models/model_19_3.pth: 100%|██████████| 100/100 [09:21<00:00,  5.62s/it]\n",
      "epoch 99 current best 0.992% error 1.292% ./models/model_19_4.pth: 100%|██████████| 100/100 [09:25<00:00,  5.66s/it]\n",
      "epoch 99 current best 1.083% error 1.808% ./models/model_17_0.pth: 100%|██████████| 100/100 [09:18<00:00,  5.59s/it]\n",
      "epoch 99 current best 1.083% error 1.792% ./models/model_17_1.pth: 100%|██████████| 100/100 [09:19<00:00,  5.60s/it]\n",
      "epoch 99 current best 1.100% error 1.800% ./models/model_17_2.pth: 100%|██████████| 100/100 [09:18<00:00,  5.58s/it]\n",
      "epoch 99 current best 1.033% error 1.908% ./models/model_17_3.pth: 100%|██████████| 100/100 [09:18<00:00,  5.59s/it]\n",
      "epoch 99 current best 1.108% error 1.683% ./models/model_17_4.pth: 100%|██████████| 100/100 [09:15<00:00,  5.55s/it]\n",
      "epoch 99 current best 1.225% error 1.675% ./models/model_15_0.pth: 100%|██████████| 100/100 [09:30<00:00,  5.71s/it]\n",
      "epoch 99 current best 1.150% error 1.825% ./models/model_15_1.pth: 100%|██████████| 100/100 [09:24<00:00,  5.65s/it]\n",
      "epoch 99 current best 1.258% error 1.617% ./models/model_15_2.pth: 100%|██████████| 100/100 [09:24<00:00,  5.65s/it]\n",
      "epoch 99 current best 1.250% error 1.667% ./models/model_15_3.pth: 100%|██████████| 100/100 [09:32<00:00,  5.72s/it]\n",
      "epoch 99 current best 1.283% error 1.750% ./models/model_15_4.pth: 100%|██████████| 100/100 [09:34<00:00,  5.74s/it]\n"
     ]
    }
   ],
   "source": [
    "### comment/uncomment the following code for training all models\n",
    "\n",
    "for target_size in TARGET_SIZES:\n",
    "    for column in range(5):\n",
    "        data = get_datasets(target_size=target_size)\n",
    "        dataloaders = get_dataloaders(data)\n",
    "\n",
    "        device = \"cuda:0\"\n",
    "        epochs = 100\n",
    "        model = MCDNN().to(device)\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "        train_model(dataloaders, model, loss_func, optimizer, epochs=epochs, device=device, save_func=save_func(target_size, column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate (generate table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate probability column of a model\n",
    "def get_column(input_image, model, target_size, device):\n",
    "    transform = get_transforms(target_size, is_train=False)\n",
    "    input_tensor = transform(input_image)[None,...]\n",
    "    with torch.no_grad():\n",
    "        column = model(input_tensor.to(device)).cpu()\n",
    "    return column\n",
    "\n",
    "def get_error_rate(prediction, ground_truth):\n",
    "    assert len(prediction) == len(ground_truth)\n",
    "    n_error = sum([1 if x != y else 0 for x, y in zip(prediction, ground_truth)])\n",
    "    return n_error / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.045 sec\n",
      "500 20.725 sec\n",
      "1000 41.040 sec\n",
      "1500 61.340 sec\n",
      "2000 81.621 sec\n",
      "2500 101.774 sec\n",
      "3000 122.051 sec\n",
      "3500 142.447 sec\n",
      "4000 162.721 sec\n",
      "4500 183.017 sec\n",
      "5000 203.286 sec\n",
      "5500 223.681 sec\n",
      "6000 244.080 sec\n",
      "6500 264.349 sec\n",
      "7000 284.744 sec\n",
      "7500 305.222 sec\n",
      "8000 325.836 sec\n",
      "8500 346.368 sec\n",
      "9000 367.641 sec\n",
      "9500 388.360 sec\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset without any transform, we will transform using above helper function\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=None, target_transform=None, download=True)\n",
    "\n",
    "# Load all 35 saved models\n",
    "models = {}\n",
    "device = \"cuda:0\"\n",
    "for target_size in TARGET_SIZES:\n",
    "    for column in range(5):\n",
    "        model_id = f'{target_size}_{column}'\n",
    "        model_path = f'./models/model_{model_id}.pth'\n",
    "        model = MCDNN()\n",
    "        model.load_state_dict(torch.load(model_path)['state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        models[model_id] = model\n",
    "\n",
    "# generate all predictions on test dataset\n",
    "ground_truth = [y for x, y in test_dataset]\n",
    "predictions = {k: [] for k, v in models.items()}\n",
    "predictions.update({f'{target_size}': [] for target_size in TARGET_SIZES})\n",
    "predictions['all'] = []\n",
    "\n",
    "start_time = time.time()\n",
    "for idx, (x, y) in enumerate(test_dataset):\n",
    "    columns = {k: None for k, v in models.items()}\n",
    "    all_net_avg = 0\n",
    "    ### switch between the following two lines to test and to run all models\n",
    "    # for target_size in [29]: #, 27, 25, 21, 19, 17, 15]: \n",
    "    for target_size in TARGET_SIZES:\n",
    "        five_net_avg = 0\n",
    "        for column in range(5):\n",
    "            model_id = f'{target_size}_{column}'\n",
    "            column = get_column(x, models[model_id], target_size, device)\n",
    "            columns[model_id] = column\n",
    "            predictions[model_id].append( torch.argmax(column).item() ) # argmax works here because there is only ONE sample\n",
    "            \n",
    "            five_net_avg += column\n",
    "            all_net_avg += column\n",
    "\n",
    "        five_net_avg /= 5 # take 5-column average\n",
    "        predictions[f'{target_size}'].append( torch.argmax(five_net_avg).item() )\n",
    "\n",
    "    all_net_avg /= 35 # take 35-column average\n",
    "    predictions['all'].append( torch.argmax(all_net_avg).item() )\n",
    "    \n",
    "    if idx % 500 == 0:\n",
    "        print(idx, f'{time.time()-start_time:.3f} sec')\n",
    "\n",
    "## save results after a long experiment so that we could run analysis later\n",
    "## always save raw results so that we could run different analysis later\n",
    "with open('./results/predictions_with_augmentation.pkl', 'wb') as f:\n",
    "    pickle.dump((predictions, ground_truth), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29_0 0.59%\n",
      "29_1 0.46%\n",
      "29_2 0.52%\n",
      "29_3 0.47%\n",
      "29_4 0.47%\n",
      "27_0 0.55%\n",
      "27_1 0.46%\n",
      "27_2 0.46%\n",
      "27_3 0.48%\n",
      "27_4 0.44%\n",
      "25_0 0.55%\n",
      "25_1 0.50%\n",
      "25_2 0.51%\n",
      "25_3 0.54%\n",
      "25_4 0.49%\n",
      "21_0 0.62%\n",
      "21_1 0.58%\n",
      "21_2 0.67%\n",
      "21_3 0.57%\n",
      "21_4 0.64%\n",
      "19_0 0.51%\n",
      "19_1 0.66%\n",
      "19_2 0.57%\n",
      "19_3 0.63%\n",
      "19_4 0.68%\n",
      "17_0 0.59%\n",
      "17_1 0.69%\n",
      "17_2 0.65%\n",
      "17_3 0.65%\n",
      "17_4 0.60%\n",
      "15_0 0.81%\n",
      "15_1 0.61%\n",
      "15_2 0.68%\n",
      "15_3 0.76%\n",
      "15_4 0.67%\n",
      "29 0.43%\n",
      "27 0.44%\n",
      "25 0.42%\n",
      "21 0.57%\n",
      "19 0.49%\n",
      "17 0.51%\n",
      "15 0.57%\n",
      "all 0.39%\n"
     ]
    }
   ],
   "source": [
    "# load experiment results to run analysis, independent from above code \n",
    "with open('./results/predictions_with_augmentation.pkl', 'rb') as f:\n",
    "    predictions, ground_truth = pickle.load(f)\n",
    "\n",
    "def print_error_rate(model_id):\n",
    "    error_rate = get_error_rate(predictions[model_id], ground_truth)\n",
    "    print(f'{model_id} {error_rate*100:.2f}%')\n",
    "\n",
    "for target_size in TARGET_SIZES:\n",
    "    for column in range(5):\n",
    "        print_error_rate( f'{target_size}_{column}' )\n",
    "        \n",
    "for target_size in TARGET_SIZES:\n",
    "    print_error_rate( f'{target_size}' )\n",
    "\n",
    "print_error_rate( 'all' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
